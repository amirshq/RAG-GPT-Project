{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling with GPT Models\n",
    "\n",
    "This notebook explores the integration of GPT models with external functions. We'll demonstrate how to call functions within the GPT context, convert Python functions to a JSON-compatible format, and execute functions retrieved from GPT outputs.\n",
    "\n",
    "### Content Overview:\n",
    "\n",
    "**I. [Understanding Function Calls in GPT](#section_one)** </br>\n",
    ">- Explore how GPT models can interpret and utilize JSON-formatted functions.\n",
    ">- Learn about the necessary components of a JSON function for GPT compatibility.\n",
    "\n",
    "**II. [Python to JSON Function Conversion](#section_two)** </br>\n",
    ">- Step-by-step guide to transforming a Python function into a JSON format.\n",
    ">- Methods for passing JSON functions to a GPT model for processing.\n",
    "\n",
    "**III. [Executing Functions from GPT Outputs](#section_three)** </br>\n",
    ">- Detailed example on how to retrieve a function from GPT output and execute it in Python.\n",
    ">- A good practice for handling and validating the execution of functions from GPT.\n",
    "\n",
    "**References:**\n",
    ">- https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb by Jeremy Howard\n",
    ">- https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling?tabs=python\n",
    "\n",
    "**NOTE:**\n",
    "- This code requires pydantic==2.5.1 for successful exection.\n",
    "\n",
    "### OpenAI chat completion arguments:\n",
    "\n",
    "Below is a sample code snippet that demonstrates how to use the OpenAI Chat Completion API with function calling:\n",
    "\n",
    "```\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=gpt_model, # Name of the gpt model : str\n",
    "    messages=messages, # Input messages: List[Dict]\n",
    "    functions=search_hotels_function_json, # List of functions in JSON format: List\n",
    "    function_call=\"auto\", # 'auto': GPT model decides when to use functions. None: GPT model will not use the functions. Dict {\"name\": \"function_name\"}: GPT model will always use the defined functions.\n",
    "    temperature=temperature # 0 to 1: float\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pydantic import create_model\n",
    "import inspect, json\n",
    "from inspect import Parameter\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the necessary configs\n",
    "gpt_model = \"gpt-35-turbo-16k\"\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_one></a>\n",
    "**Understanding Function Calls in GPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_hotels_function_json = [  \n",
    "    {\n",
    "        \"name\": \"search_hotels\",\n",
    "        \"description\": \"Retrieves hotels from the search index based on the parameters provided\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The location of the hotel (i.e. Seattle, WA)\"\n",
    "                },\n",
    "                \"max_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The maximum price for the hotel\"\n",
    "                },\n",
    "                \"features\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A comma separated list of features (i.e. beachfront, free wifi, etc.)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages= [\n",
    "    {\"role\": \"user\", \"content\": \"Find beachfront hotels in San Diego for less than $300 a month with free breakfast.\"}\n",
    "]\n",
    "\n",
    "# Pass th function toGPT model\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=gpt_model,\n",
    "    messages=messages,\n",
    "    functions=search_hotels_function_json,\n",
    "    function_call=\"auto\",\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"function_call\": {\n",
      "    \"name\": \"search_hotels\",\n",
      "    \"arguments\": \"{\\n  \\\"location\\\": \\\"San Diego\\\",\\n  \\\"max_price\\\": 300,\\n  \\\"features\\\": \\\"beachfront,free breakfast\\\"\\n}\"\n",
      "  }\n",
      "} \n",
      "\n",
      "\n",
      "Function call:\n",
      "{\n",
      "  \"name\": \"search_hotels\",\n",
      "  \"arguments\": \"{\\n  \\\"location\\\": \\\"San Diego\\\",\\n  \\\"max_price\\\": 300,\\n  \\\"features\\\": \\\"beachfront,free breakfast\\\"\\n}\"\n",
      "} \n",
      "\n",
      "\n",
      "Function call arguments:\n",
      "{\n",
      "  \"location\": \"San Diego\",\n",
      "  \"max_price\": 300,\n",
      "  \"features\": \"beachfront,free breakfast\"\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Full response:\")\n",
    "print(response.choices[0].message, \"\\n\")\n",
    "print(\"\\nFunction call:\")\n",
    "print(response.choices[0].message.function_call, \"\\n\")\n",
    "print(\"\\nFunction call arguments:\")\n",
    "print(response.choices[0].message.function_call.arguments, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_two></a>\n",
    "**Python to JSON Function Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\froozitalab\\AppData\\Local\\Temp\\ipykernel_5244\\2071725937.py:8: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'sum',\n",
       " 'description': 'Adds a + b',\n",
       " 'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
       "   'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n",
       "  'required': ['a'],\n",
       "  'title': 'Input for `sum`',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum(a:int, b:int=1):\n",
    "    \"Adds a + b\"\n",
    "    return a + b\n",
    "\n",
    "def jsonschema(f):\n",
    "    kw = {n:(o.annotation, ... if o.default==Parameter.empty else o.default)\n",
    "          for n,o in inspect.signature(f).parameters.items()}\n",
    "    s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
    "    return dict(name=f.__name__, description=f.__doc__, parameters=s)\n",
    "\n",
    "print(type(jsonschema(sum)), \"\\n\")\n",
    "jsonschema(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When functions are provided, by default the function_call will be set to \"auto\" and the model will decide whether or not a function should be called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\froozitalab\\AppData\\Local\\Temp\\ipykernel_5244\\2071725937.py:8: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "llm_system_role = \"Answer the user's question.\"\n",
    "prompt = \"What is 6+3?\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "            engine=gpt_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": llm_system_role},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            functions=[jsonschema(sum)],\n",
    "            function_call={\"name\": \"sum\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"function_call\": {\n",
      "    \"name\": \"sum\",\n",
      "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Function call:\n",
      "{\n",
      "  \"name\": \"sum\",\n",
      "  \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
      "}\n",
      "\n",
      "Function call arguments:\n",
      "{\n",
      "  \"a\": 6,\n",
      "  \"b\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Full response:\")\n",
    "print(response.choices[0].message, \"\\n\")\n",
    "print(\"\\nFunction call:\")\n",
    "print(response.choices[0].message.function_call, \"\\n\")\n",
    "print(\"\\nFunction call arguments:\")\n",
    "print(response.choices[0].message.function_call.arguments, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs_ok = {'sums', 'python'}\n",
    "\n",
    "def call_func(response):\n",
    "    function = response.choices[0].message.function_call\n",
    "    if function.name not in funcs_ok: return print(f'Not allowed: {function.name}')\n",
    "    f = globals()[function.name]\n",
    "    return f(**json.loads(function.arguments))\n",
    "\n",
    "call_func(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"function_call\": {\n",
      "    \"name\": \"sums\",\n",
      "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Function call arguments:\n",
      "{\n",
      "  \"a\": 6,\n",
      "  \"b\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Full response:\")\n",
    "print(response.choices[0].message)\n",
    "print(\"\\nFunction call arguments:\")\n",
    "print(response.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_three></a>\n",
    "**Executing Functions from GPT Outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def run(code):\n",
    "    \"\"\"\n",
    "    Executes the given Python code and returns the result of the last expression.\n",
    "    Args:\n",
    "        code (str): The Python code to execute.\n",
    "    Returns:\n",
    "        The result of the last expression in the code, or None if there is no expression.\n",
    "    Raises:\n",
    "        SyntaxError: If the code contains syntax errors.\n",
    "        TypeError: If the code is not a string.\n",
    "    \"\"\"\n",
    "    tree = ast.parse(code)\n",
    "    last_node = tree.body[-1] if tree.body else None\n",
    "    \n",
    "    # If the last node is an expression, modify the AST to capture the result\n",
    "    if isinstance(last_node, ast.Expr):\n",
    "        tgts = [ast.Name(id='_result', ctx=ast.Store())]\n",
    "        assign = ast.Assign(targets=tgts, value=last_node.value)\n",
    "        tree.body[-1] = ast.fix_missing_locations(assign)\n",
    "\n",
    "    ns = {}\n",
    "    exec(compile(tree, filename='<ast>', mode='exec'), ns)\n",
    "    return ns.get('_result', None)\n",
    "\n",
    "def python(code:str):\n",
    "    \"Return result of executing `code` using python. If execution not permitted, returns `#FAIL#`\"\n",
    "    go = input(f'Proceed with execution?\\n```\\n{code}\\n```\\n')\n",
    "    if go.lower()!='y': return '#FAIL#'\n",
    "    return run(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"\"\"\n",
    "a=2\n",
    "b=4\n",
    "a**b\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "12 factorial, denoted as 12!, is the product of all positive integers from 1 to 12. Mathematically, it can be calculated as:\n",
      "\n",
      "12! = 12 × 11 × 10 × 9 × 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1\n",
      "\n",
      "Simplifying this expression, we get:\n",
      "\n",
      "12! = 479,001,600\n",
      "\n",
      "Therefore, 12 factorial is equal to 479,001,600.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is 12 factorial?\"\n",
    "llm_system_role = \"You are a math expert. Use your best knowledge to answer the question.\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "            engine=GPT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": llm_system_role},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=TEMPERATURE,\n",
    "        )\n",
    "\n",
    "print(\"Full response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given result: 479,001,600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"function_call\": {\n",
      "    \"name\": \"python\",\n",
      "    \"arguments\": \"{\\n  \\\"code\\\": \\\"import math\\\\nmath.factorial(12)\\\"\\n}\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Function call arguments:\n",
      "{\n",
      "  \"code\": \"import math\\nmath.factorial(12)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is 12 factorial?\"\n",
    "llm_system_role = \"Use python for any required computations.\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "            engine=GPT_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": llm_system_role},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=TEMPERATURE,\n",
    "            functions=[jsonschema(python)],\n",
    "            function_call=\"auto\"\n",
    "        )\n",
    "\n",
    "print(\"Full response:\")\n",
    "print(response.choices[0].message)\n",
    "print(\"\\nFunction call arguments:\")\n",
    "print(response.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8MJytC90aoWYqWOjZmztdWDBBctnT at 0x16c86333470> JSON: {\n",
       "  \"id\": \"chatcmpl-8MJytC90aoWYqWOjZmztdWDBBctnT\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1700330715,\n",
       "  \"model\": \"gpt-35-turbo-16k\",\n",
       "  \"prompt_filter_results\": [\n",
       "    {\n",
       "      \"prompt_index\": 0,\n",
       "      \"content_filter_results\": {\n",
       "        \"hate\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"self_harm\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"sexual\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"violence\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"function_call\",\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"function_call\": {\n",
       "          \"name\": \"python\",\n",
       "          \"arguments\": \"{\\n  \\\"code\\\": \\\"import math\\\\nmath.factorial(12)\\\"\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"content_filter_results\": {}\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 72,\n",
       "    \"completion_tokens\": 21,\n",
       "    \"total_tokens\": 93\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479001600"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_func(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
